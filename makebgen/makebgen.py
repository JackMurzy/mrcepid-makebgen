#!/usr/bin/env python
# mrcepid-collapsevariants 0.0.1
# Generated by dx-app-wizard.
#
# Author: Eugene Gardner (eugene.gardner at mrc.epid.cam.ac.uk)
#
# DNAnexus Python Bindings (dxpy) documentation:
#   http://autodoc.dnanexus.com/bindings/python/current/
import csv
from pathlib import Path
from typing import Dict

import dxpy
from general_utilities.association_resources import check_gzipped
from general_utilities.import_utils.file_handlers.dnanexus_utilities import generate_linked_dx_file
from general_utilities.import_utils.file_handlers.input_file_handler import InputFileHandler
from general_utilities.job_management.thread_utility import ThreadUtility
from general_utilities.mrc_logger import MRCLogger

from makebgen.process_bgen.process_bgen import make_final_bgen, make_bgen_from_vcf
from makebgen.scripts.chunking_helper import chunking_helper

LOGGER = MRCLogger().get_logger()


def process_one_batch(batch: list, batch_index: int,
                      make_bcf: bool, output_prefix: str) -> Dict[str, Dict]:
    """
    A function to process a batch of chunked files, converting BCF files to BGEN format and merging them.
    :param batch: A list of chunked files to process.
    :param batch_index: An index for the batch, used in naming output files.
    :param make_bcf: Should a concatenated BCF be made in addition to the BGEN?
    :param output_prefix: The prefix for the output files.
    :return: A list of dictionaries containing the output files for the batch.
    """
    LOGGER.info(f"Processing batch {batch_index} with {len(batch)} chunked files...")

    chunk_threads = ThreadUtility(
        incrementor=1,
        thread_factor=3,
        error_message='chunk-level bgen creation failed'
    )

    for i, chunk_file in enumerate(batch):
        chunk_threads.launch_job(
            process_single_chunk,
            chunk_file=chunk_file,
            chunk_index=i,
            batch_index=batch_index,
            make_bcf=make_bcf,
            output_prefix=output_prefix
        )

    results_list = list(chunk_threads)

    # And gather the resulting futures which are returns of all bgens we need to concatenate:
    bgen_prefixes = {}
    for result in results_list:
        bgen_prefixes[result['vcfprefix']] = result['start']

    LOGGER.info(f"All chunks done for batch {batch_index}, merging...")

    merged = make_final_bgen(bgen_prefixes=bgen_prefixes, output_prefix=f"{output_prefix}_{batch_index}",
                             make_bcf=make_bcf)

    # Set output
    output = {'bgen': dxpy.dxlink(generate_linked_dx_file(merged['bgen']['file'])),
              'index': dxpy.dxlink(generate_linked_dx_file(merged['bgen']['index'])),
              'sample': dxpy.dxlink(generate_linked_dx_file(merged['bgen']['sample'])),
              'vep': dxpy.dxlink(generate_linked_dx_file(merged['vep']['file'])),
              'vep_idx': dxpy.dxlink(generate_linked_dx_file(merged['vep']['index']))}

    # Delete all chunk-level temporary files
    for result in results_list:
        for path_key in result:
            if isinstance(result[path_key], Path) and result[path_key].exists():
                LOGGER.debug(f"Deleting result file: {result[path_key]}")
                result[path_key].unlink()

    return output


def process_single_chunk(chunk_file: Path, chunk_index: int,
                         batch_index: int, make_bcf: bool, output_prefix: str) -> dict:
    """
    Convert all VCFs in a chunk file to BGENs, then merge into one BGEN for that chunk.

    :param chunk_file: The file containing the chunk of coordinates to process.
    :param chunk_index: The index of the chunk being processed.
    :param batch_index: The index of the batch this chunk belongs to.
    :param make_bcf: Should a concatenated BCF be made in addition to the BGEN?
    :param output_prefix: The prefix for the output files.
    :return: A dictionary containing the output files for the chunk.
    """
    LOGGER.info(f"Starting processing of chunk {chunk_index} in batch {batch_index}")

    with check_gzipped(chunk_file) as coord_file:
        coord_reader = csv.DictReader(coord_file, delimiter="\t")

        thread_utility = ThreadUtility(
            incrementor=20,  # try 10?
            thread_factor=4,  # or 4?
            error_message='bcf to bgen thread failed'
        )

        previous_vep_id = None
        bgen_inputs = {}

        for row in coord_reader:
            thread_utility.launch_job(
                make_bgen_from_vcf,
                vcf_id=row['output_bcf'],
                vep_id=row['output_vep'],
                previous_vep_id=previous_vep_id,
                start=row['start'],
                make_bcf=make_bcf
            )
            previous_vep_id = row['output_vep']

        results_list = list(thread_utility)

        for result in results_list:
            bgen_inputs[result['vcfprefix']] = result['start']

    output_prefix = f"{output_prefix}_batch{batch_index}_chunk{chunk_index}"
    final_files = make_final_bgen(bgen_inputs, output_prefix, make_bcf)

    output = {
        'bgen': final_files['bgen']['file'],
        'bgen_index': final_files['bgen']['index'],
        'sample': final_files['bgen']['sample'],
        'vep': final_files['vep']['file'],
        'vep_index': final_files['vep']['index']
    }

    if final_files['bcf']['file'] is not None:
        output['bcf'] = dxpy.dxlink(generate_linked_dx_file(final_files['bcf']['file']))
        output['bcf_idx'] = dxpy.dxlink(generate_linked_dx_file(final_files['bcf']['index']))

    output['vcfprefix'] = output_prefix
    output['start'] = row['start']

    LOGGER.info(f"Finished chunk {chunk_index} in batch {batch_index}")

    # Delete only intermediate files with known extensions
    with check_gzipped(chunk_file) as coord_file:
        coord_reader = csv.DictReader(coord_file, delimiter="\t")
        for row in coord_reader:
            prefix = row['vcf_prefix']
            for suffix in [".bcf", ".bcf.csi", ".vep.tsv.gz", ".vep.tsv.gz.tbi"]:
                file = Path(f"{prefix}{suffix}")
                if file.exists():
                    try:
                        file.unlink()
                        LOGGER.debug(f"Deleted intermediate file: {file}")
                    except Exception as e:
                        LOGGER.warning(f"Failed to delete file {file}: {e}")

    return output


@dxpy.entry_point('main')
def main(output_prefix: str, coordinate_file: str, make_bcf: bool, gene_dict: str,
         size_of_bgen: int) -> dict:
    """Main entry point into this applet. This function initiates the conversion of all bcf files for a given chromosome
    into a single .bgen file.

    Coordinate file must have the following columns:

        chrom   start   end     vcf_prefix      output_bcf      output_bcf_idx  output_vep      output_vep_idx

    :param output_prefix: Output prefix. Output file will be named <output_prefix>.bgen
    :param coordinate_file: A file containing the coordinates of all bcf files to be processed.
    :param make_bcf: Should a concatenated bcf be made in addition to the bgen?
    :param gene_dict: A file containing the gene dictionary to be used for chunking.
    :param size_of_bgen: The number of chunks to concatenate into a single bgen file.
    :return: An output dictionary following DNANexus conventions.
    """

    # start the file parser class and get the coordinates file
    # Get coordinate and gene files
    coordinates = InputFileHandler(coordinate_file)
    coordinate_path = coordinates.get_file_handle()

    gene_dict_file = InputFileHandler(gene_dict)
    gene_dict_path = gene_dict_file.get_file_handle()

    # Run chunking and generate BGEN chunk files
    chunked_files = chunking_helper(
        gene_dict=gene_dict_path,
        coordinate_path=coordinate_path,
        chunk_size=3,
        output_path="chunked_coordinates"
    )

    batch_size = size_of_bgen

    final_outputs = []

    num_batches = (len(chunked_files) + batch_size - 1) // batch_size
    LOGGER.info(f"Total number of batches: {num_batches}")

    for i in range(0, len(chunked_files), batch_size):
        batch = chunked_files[i:i + batch_size]
        batch_index = i // batch_size + 1

        LOGGER.info(f"Starting batch {batch_index} of {num_batches}")
        batch_outputs = process_one_batch(
            batch=batch,
            batch_index=batch_index,
            make_bcf=make_bcf,
            output_prefix=output_prefix
        )
        final_outputs.extend(batch_outputs)
        LOGGER.info(f"Finished batch {batch_index}")

    return {"final_outputs": final_outputs}


dxpy.run()
