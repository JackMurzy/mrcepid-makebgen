#!/usr/bin/env python
# mrcepid-collapsevariants 0.0.1
# Generated by dx-app-wizard.
#
# Author: Eugene Gardner (eugene.gardner at mrc.epid.cam.ac.uk)
#
# DNAnexus Python Bindings (dxpy) documentation:
#   http://autodoc.dnanexus.com/bindings/python/current/
import csv

import dxpy
from general_utilities.association_resources import check_gzipped
from general_utilities.import_utils.file_handlers.dnanexus_utilities import generate_linked_dx_file
from general_utilities.import_utils.file_handlers.input_file_handler import InputFileHandler
from general_utilities.job_management.thread_utility import ThreadUtility
from general_utilities.mrc_logger import MRCLogger

from makebgen.process_bgen.process_bgen import make_final_bgen, make_bgen_from_vcf
from makebgen.scripts.chunking_helper import chunking_helper
from makebgen.scripts.concat_bgens import chunk_dict_reader, process_chunk_group

LOGGER = MRCLogger().get_logger()


@dxpy.entry_point('main')
def main(output_prefix: str, coordinate_file: str, make_bcf: bool, gene_dict: str,
         size_of_bgen: int) -> dict:
    """Main entry point into this applet. This function initiates the conversion of all bcf files for a given chromosome
    into a single .bgen file.

    Coordinate file must have the following columns:

        chrom   start   end     vcf_prefix      output_bcf      output_bcf_idx  output_vep      output_vep_idx

    :param output_prefix: Output prefix. Output file will be named <output_prefix>.bgen
    :param coordinate_file: A file containing the coordinates of all bcf files to be processed.
    :param make_bcf: Should a concatenated bcf be made in addition to the bgen?
    :param gene_dict: A file containing the gene dictionary to be used for chunking.
    :param size_of_bgen: The number of chunks to concatenate into a single bgen file.
    :return: An output dictionary following DNANexus conventions.
    """

    # start the file parser class and get the coordinates file
    coordinates = InputFileHandler(coordinate_file)
    coordinate_path = coordinates.get_file_handle()

    # start the file parser class and get the gene dictionary file
    gene_dict = InputFileHandler(gene_dict)
    gene_dict_path = gene_dict.get_file_handle()

    # run the chunking script and save the output to a directory
    chunked_files = chunking_helper(gene_dict=gene_dict_path, coordinate_path=coordinate_path,
                                    chunk_size=3, output_path="chunked_coordinates")

    batch_size = size_of_bgen
    final_outputs = []

    # Process files in batches
    for i in range(0, len(chunked_files), batch_size):
        batch = chunked_files[i:i + batch_size]
        batch_index = i // batch_size + 1

        # Example: Print or run a command on each batch
        print(f"Processing batch {i // batch_size + 1}: {[str(f) for f in batch]}")

        # Get the processed coordinate file
        total_bcf = 0

        # start the file parser class and get the coordinates file
        coordinates = InputFileHandler(coordinate_file)
        coordinate_path = coordinates.get_file_handle()

        with check_gzipped(coordinate_path) as coord_file:
            coord_file_reader = csv.DictReader(coord_file, delimiter="\t")

            thread_utility = ThreadUtility(incrementor=100,
                                           thread_factor=2,
                                           error_message='A bcf to bgen thread failed')
            previous_vep_id = None
            for row in coord_file_reader:
                total_bcf += 1
                thread_utility.launch_job(make_bgen_from_vcf,
                                          vcf_id=row['output_bcf'],
                                          vep_id=row['output_vep'],
                                          previous_vep_id=previous_vep_id,
                                          start=row['start'],
                                          make_bcf=make_bcf
                                          )
                previous_vep_id = row['output_vep']

            # And gather the resulting futures which are returns of all bgens we need to concatenate:
            bgen_prefixes = {}
            for result in thread_utility:
                bgen_prefixes[result['vcfprefix']] = result['start']

        # Convert all input bcfs to bgen
        LOGGER.info(f'Converting {total_bcf} bcf(s) to single bgen')

        # Now mash all the bgen files together
        LOGGER.info(f'Merging bgen files into {output_prefix}.bgen...')
        final_files = make_final_bgen(bgen_prefixes, output_prefix, make_bcf)

        # Set output
        output = {'bgen': final_files['bgen']['file'],
                  'index': final_files['bgen']['index'],
                  'sample': final_files['bgen']['sample'],
                  'vep': final_files['vep']['file'],
                  'vep_idx': final_files['vep']['index']}

        if final_files['bcf']['file'] is not None:
            output['bcf'] = dxpy.dxlink(generate_linked_dx_file(final_files['bcf']['file']))
            output['bcf_idx'] = dxpy.dxlink(generate_linked_dx_file(final_files['bcf']['index']))

        # Now mash all the small bgen files together into one large one
        LOGGER.info(f'Merging 3 bgen files into a large BGEN...')

        final_outputs = []

        for chunk_index, chunk in enumerate(chunk_dict_reader(output, chunk_size=size_of_bgen), start=1):
            # process the chunk group
            merged = process_chunk_group(batch_index=batch_index, chunk=chunk)

            linked_output = {
                'bgen': dxpy.dxlink(generate_linked_dx_file(merged['bgen'])),
                'index': dxpy.dxlink(generate_linked_dx_file(merged['bgen_index'])),
                'sample': dxpy.dxlink(generate_linked_dx_file(merged['sample'])),
                'vep': dxpy.dxlink(generate_linked_dx_file(merged['vep'])),
                'vep_idx': dxpy.dxlink(generate_linked_dx_file(merged['vep_index']))
            }

            final_outputs.append(linked_output)

    return final_outputs


dxpy.run()
