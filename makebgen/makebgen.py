#!/usr/bin/env python
# mrcepid-collapsevariants 0.0.1
# Generated by dx-app-wizard.
#
# Author: Eugene Gardner (eugene.gardner at mrc.epid.cam.ac.uk)
#
# DNAnexus Python Bindings (dxpy) documentation:
#   http://autodoc.dnanexus.com/bindings/python/current/
import csv
from pathlib import Path
import dxpy
from general_utilities.association_resources import check_gzipped
from general_utilities.import_utils.file_handlers.dnanexus_utilities import generate_linked_dx_file
from general_utilities.import_utils.file_handlers.input_file_handler import InputFileHandler
from general_utilities.job_management.thread_utility import ThreadUtility
from general_utilities.mrc_logger import MRCLogger

from makebgen.process_bgen.process_bgen import make_final_bgen, make_bgen_from_vcf
from makebgen.scripts.chunking_helper import chunking_helper
from makebgen.scripts.concat_bgens import process_chunk_group

LOGGER = MRCLogger().get_logger()


def process_one_batch(batch: list, batch_index: int,
                      make_bcf: bool, output_prefix: str) -> list:
    """
    A function to process a batch of chunked files, converting BCF files to BGEN format and merging them.
    :param batch: A list of chunked files to process.
    :param batch_index: An index for the batch, used in naming output files.
    :param make_bcf: Should a concatenated BCF be made in addition to the BGEN?
    :param output_prefix: The prefix for the output files.
    :return: A list of dictionaries containing the output files for the batch.
    """
    LOGGER.info(f"Processing batch {batch_index} with {len(batch)} chunked files...")

    merged_chunks = []

    for index, chunk_file in enumerate(batch):
        with check_gzipped(chunk_file) as coord_file:
            coord_reader = csv.DictReader(coord_file, delimiter="\t")

            thread_utility = ThreadUtility(
                incrementor=100,
                thread_factor=2,
                error_message='bcf to bgen thread failed'
            )

            previous_vep_id = None
            bgen_inputs = {}

            for row in coord_reader:
                thread_utility.launch_job(
                    make_bgen_from_vcf,
                    vcf_id=row['output_bcf'],
                    vep_id=row['output_vep'],
                    previous_vep_id=previous_vep_id,
                    start=row['start'],
                    make_bcf=make_bcf
                )
                previous_vep_id = row['output_vep']

            for result in thread_utility:
                bgen_inputs[result['vcfprefix']] = result['start']

        final_files = make_final_bgen(bgen_inputs, f"{output_prefix}_{index}", make_bcf)

        output = {
            'bgen': final_files['bgen']['file'],
            'bgen_index': final_files['bgen']['index'],
            'sample': final_files['bgen']['sample'],
            'vep': final_files['vep']['file'],
            'vep_index': final_files['vep']['index']
        }

        if final_files['bcf']['file'] is not None:
            output['bcf'] = dxpy.dxlink(generate_linked_dx_file(final_files['bcf']['file']))
            output['bcf_idx'] = dxpy.dxlink(generate_linked_dx_file(final_files['bcf']['index']))

        merged_chunks.append(output)

    # All 3 chunks done â€” now merge them
    merged = process_chunk_group(batch_index=batch_index, chunk=merged_chunks)

    # Upload and cleanup
    linked_output = {
        'bgen': dxpy.dxlink(generate_linked_dx_file(merged['bgen'])),
        'index': dxpy.dxlink(generate_linked_dx_file(merged['bgen_index'])),
        'sample': dxpy.dxlink(generate_linked_dx_file(merged['sample'])),
        'vep': dxpy.dxlink(generate_linked_dx_file(merged['vep'])),
        'vep_idx': dxpy.dxlink(generate_linked_dx_file(merged['vep_index']))
    }

    for f in merged.values():
        if isinstance(f, Path) and f.exists():
            f.unlink()

    return [linked_output]


@dxpy.entry_point('main')
def main(output_prefix: str, coordinate_file: str, make_bcf: bool, gene_dict: str,
         size_of_bgen: int) -> dict:
    """Main entry point into this applet. This function initiates the conversion of all bcf files for a given chromosome
    into a single .bgen file.

    Coordinate file must have the following columns:

        chrom   start   end     vcf_prefix      output_bcf      output_bcf_idx  output_vep      output_vep_idx

    :param output_prefix: Output prefix. Output file will be named <output_prefix>.bgen
    :param coordinate_file: A file containing the coordinates of all bcf files to be processed.
    :param make_bcf: Should a concatenated bcf be made in addition to the bgen?
    :param gene_dict: A file containing the gene dictionary to be used for chunking.
    :param size_of_bgen: The number of chunks to concatenate into a single bgen file.
    :return: An output dictionary following DNANexus conventions.
    """

    # start the file parser class and get the coordinates file
    # Get coordinate and gene files
    coordinates = InputFileHandler(coordinate_file)
    coordinate_path = coordinates.get_file_handle()

    gene_dict_file = InputFileHandler(gene_dict)
    gene_dict_path = gene_dict_file.get_file_handle()

    # Run chunking and generate BGEN chunk files
    chunked_files = chunking_helper(
        gene_dict=gene_dict_path,
        coordinate_path=coordinate_path,
        chunk_size=3,
        output_path="chunked_coordinates"
    )

    batch_size = size_of_bgen

    final_outputs = []

    for i in range(0, len(chunked_files), batch_size):
        batch = chunked_files[i:i + batch_size]
        batch_index = i // batch_size + 1

        LOGGER.info(f"Starting batch {batch_index}...")
        batch_outputs = process_one_batch(
            batch=batch,
            batch_index=batch_index,
            make_bcf=make_bcf,
            output_prefix=output_prefix
        )
        final_outputs.extend(batch_outputs)
        LOGGER.info(f"Finished batch {batch_index}.")

    return {"final_outputs": final_outputs}


dxpy.run()
