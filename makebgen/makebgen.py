#!/usr/bin/env python
# mrcepid-collapsevariants 0.0.1
# Generated by dx-app-wizard.
#
# Author: Eugene Gardner (eugene.gardner at mrc.epid.cam.ac.uk)
#
# DNAnexus Python Bindings (dxpy) documentation:
#   http://autodoc.dnanexus.com/bindings/python/current/
import csv
from pathlib import Path
from typing import Any

import dxpy
from general_utilities.association_resources import check_gzipped
from general_utilities.import_utils.file_handlers.dnanexus_utilities import generate_linked_dx_file
from general_utilities.import_utils.file_handlers.input_file_handler import InputFileHandler
from general_utilities.job_management.thread_utility import ThreadUtility
from general_utilities.mrc_logger import MRCLogger

from makebgen.process_bgen.process_bgen import make_final_bgen, make_bgen_from_vcf
from makebgen.scripts.chunking_helper import chunking_helper

LOGGER = MRCLogger().get_logger()


def process_one_batch(batch: list, batch_index: int,
                      make_bcf: bool, output_prefix: str) -> list[Any]:
    """
    A function to process a batch of chunked files, converting BCF files to BGEN format and merging them.
    :param batch: A list of chunked files to process.
    :param batch_index: An index for the batch, used in naming output files.
    :param make_bcf: Should a concatenated BCF be made in addition to the BGEN?
    :param output_prefix: The prefix for the output files.
    :return: A list of dictionaries containing the output files for the batch.
    """
    LOGGER.info(f"Processing batch {batch_index} with {len(batch)} chunked files...")

    chunk_threads = ThreadUtility(
        incrementor=1,
        thread_factor=3,
        error_message='chunk-level bgen creation failed'
    )

    for i, chunk_file in enumerate(batch):
        chunk_threads.launch_job(
            process_single_chunk,
            chunk_file=chunk_file,
            chunk_index=i,
            batch_index=batch_index,
            make_bcf=make_bcf,
            output_prefix=output_prefix
        )

    results_list = list(chunk_threads)
    LOGGER.info(f"All chunks done for batch {batch_index}.")
    return results_list


def process_single_chunk(chunk_file: Path, chunk_index: int,
                         batch_index: int, make_bcf: bool, output_prefix: str) -> dict:
    """
    Convert all VCFs in a chunk file to BGENs, then merge into one BGEN for that chunk.

    :param chunk_file: The file containing the chunk of coordinates to process.
    :param chunk_index: The index of the chunk being processed.
    :param batch_index: The index of the batch this chunk belongs to.
    :param make_bcf: Should a concatenated BCF be made in addition to the BGEN?
    :param output_prefix: The prefix for the output files.
    :return: A dictionary containing the output files for the chunk.
    """
    LOGGER.info(f"Starting processing of chunk {chunk_index} in batch {batch_index}")

    with check_gzipped(chunk_file) as coord_file:
        coord_reader = csv.DictReader(coord_file, delimiter="\t")

        thread_utility = ThreadUtility(
            incrementor=20,  # try 10?
            thread_factor=4,  # or 4?
            error_message='bcf to bgen thread failed'
        )

        previous_vep_id = None
        bgen_inputs = {}

        for row in coord_reader:
            thread_utility.launch_job(
                make_bgen_from_vcf,
                vcf_id=row['output_bcf'],
                vep_id=row['output_vep'],
                previous_vep_id=previous_vep_id,
                start=row['start'],
                make_bcf=make_bcf
            )
            previous_vep_id = row['output_vep']

        results_list = list(thread_utility)

        for result in results_list:
            bgen_inputs[result['vcfprefix']] = result['start']

    output_prefix = f"{output_prefix}_batch{batch_index}_chunk{chunk_index}"
    final_files = make_final_bgen(bgen_inputs, output_prefix, make_bcf)

    output = {
        'bgen': final_files['bgen']['file'],
        'bgen_index': final_files['bgen']['index'],
        'sample': final_files['bgen']['sample'],
        'vep': final_files['vep']['file'],
        'vep_index': final_files['vep']['index']
    }

    if final_files['bcf']['file'] is not None:
        output['bcf'] = dxpy.dxlink(generate_linked_dx_file(final_files['bcf']['file']))
        output['bcf_idx'] = dxpy.dxlink(generate_linked_dx_file(final_files['bcf']['index']))

    output['vcfprefix'] = output_prefix
    output['start'] = row['start']

    LOGGER.info(f"Finished chunk {chunk_index} in batch {batch_index}")

    # Delete all files with the same prefix as vcf_prefix
    with check_gzipped(chunk_file) as coord_file:
        coord_reader = csv.DictReader(coord_file, delimiter="\t")
        for row in coord_reader:
            prefix = row['vcf_prefix']
            for f in Path(".").glob(f"{prefix}*"):
                try:
                    f.unlink()
                except Exception as e:
                    LOGGER.warning(f"Failed to delete file {f}: {e}")

    return output


@dxpy.entry_point('main')
def main(output_prefix: str, coordinate_file: str, make_bcf: bool, gene_dict: str,
         size_of_bgen: int) -> dict:
    """
    Main entry point into this applet. This function initiates the conversion of all bcf files
    for a given chromosome into a single .bgen file.

    Coordinate file must have the following columns:

        chrom   start   end     vcf_prefix      output_bcf      output_bcf_idx
        output_vep      output_vep_idx

    :param output_prefix: Output prefix. Output file will be named <output_prefix>.bgen
    :param coordinate_file: A file containing the coordinates of all bcf files to be processed.
    :param make_bcf: Should a concatenated bcf be made in addition to the bgen?
    :param gene_dict: A file containing the gene dictionary to be used for chunking.
    :param size_of_bgen: The number of chunks to concatenate into a single bgen file.
    :return: A dictionary with final DX file links.
    """

    # Get coordinate and gene files
    coordinates = InputFileHandler(coordinate_file)
    coordinate_path = coordinates.get_file_handle()

    gene_dict_file = InputFileHandler(gene_dict)
    gene_dict_path = gene_dict_file.get_file_handle()

    # Chunk the input coordinates using the gene dictionary
    chunked_files = chunking_helper(
        gene_dict=gene_dict_path,
        coordinate_path=coordinate_path,
        chunk_size=3,
        output_path="chunked_coordinates"
    )

    batch_size = size_of_bgen
    num_batches = (len(chunked_files) + batch_size - 1) // batch_size
    LOGGER.info(f"Total number of batches: {num_batches}")

    all_chunk_results = []

    for i in range(0, len(chunked_files), batch_size):
        batch = chunked_files[i:i + batch_size]
        batch_index = i // batch_size + 1

        LOGGER.info(f"Starting batch {batch_index} of {num_batches}")
        batch_outputs = process_one_batch(
            batch=batch,
            batch_index=batch_index,
            make_bcf=make_bcf,
            output_prefix=output_prefix
        )
        all_chunk_results.extend(batch_outputs)
        LOGGER.info(f"Finished batch {batch_index}")

    # Merge chunk-level BGENs into one final BGEN
    bgen_prefixes = {
        result['vcfprefix']: result['start']
        for result in all_chunk_results
    }

    LOGGER.info("Merging all chunk-level BGENs into final output...")
    merged = make_final_bgen(
        bgen_prefixes=bgen_prefixes,
        output_prefix=output_prefix,
        make_bcf=make_bcf
    )

    # Link final outputs to DNAnexus
    output = {
        'bgen': dxpy.dxlink(generate_linked_dx_file(merged['bgen']['file'])),
        'index': dxpy.dxlink(generate_linked_dx_file(merged['bgen']['index'])),
        'sample': dxpy.dxlink(generate_linked_dx_file(merged['bgen']['sample'])),
        'vep': dxpy.dxlink(generate_linked_dx_file(merged['vep']['file'])),
        'vep_idx': dxpy.dxlink(generate_linked_dx_file(merged['vep']['index']))
    }

    if make_bcf and merged['bcf']['file'] is not None:
        output['bcf'] = dxpy.dxlink(generate_linked_dx_file(merged['bcf']['file']))
        output['bcf_idx'] = dxpy.dxlink(generate_linked_dx_file(merged['bcf']['index']))

    # Delete intermediate chunk-level merged files
    for result in all_chunk_results:
        for key in ['bgen', 'bgen_index', 'sample', 'vep', 'vep_index']:
            path = result.get(key)
            if isinstance(path, Path) and path.exists():
                try:
                    LOGGER.debug(f"Deleting intermediate file: {path}")
                    path.unlink()
                except Exception as e:
                    LOGGER.warning(f"Could not delete file {path}: {e}")

    return {"final_outputs": output}


dxpy.run()
